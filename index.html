<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Qiuyuan Huang</title>
    <link rel="stylesheet" href="style.css">
    <!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<script>
function UpdateTaskIllustration() {
  var task = document.getElementById("illu-task").value;
  console.log("illu", task)
  var img = document.getElementById("task-illustration");
  img.src = "assets/" + "illu-" + task + ".png"
}

function UpdateTaskDemonstration() {
  var task = document.getElementById("demo-task").value;
  console.log("demo", task)
  var video = document.getElementById("task-demonstration");
  video.src = "assets/" + "demo-" + task + ".mp4"
  video.playbackRate = 1.75;
  video.play();
}
</script>
  </head>
  
  <body>
    <div class="topnav">
      <div class="topnav-left">
        <img src="Q5.png"  width="115" height="115" alt="Microsoft Research">  
        <a href="index.html">Qiuyuan (Enno) Huang</a>
        <a href="mailto:theidfree@gmail.com"><class="button style="font-size:18px"><i class="fa fa-envelope"></i> Email</button></a>
        <a href="https://scholar.google.com/citations?user=U7Mmyc8AAAAJ&hl=en"><class="button style="font-size:18px"><i class="fa fa-book"></i> Google Scholar</button></a> 
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a>
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a> 
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a> 
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a> 
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a> 
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a> 
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a> 
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a>
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a>
      </div>
    </div>    <div class="header">
      <p></p>
    </div>
    <div class="sidenav">
      <a href="#Research Interest">Research Interest</a>
      <a href="#Talks">Recent Talks</a>
      <a href="#Publications">Publications</a>
      <a href="#ToolDownload">Data-Driven Tools</a>
      <a href="#EULA">Dataverse User License </a>
      <a href="#Fun">Fun</a>
    </div>
    <div class="main">
      <p>I am a Principal Researcher at Microsoft Research (MSR), Redmond, WA. 
        My current research interests are in deep learning in general, and embodied AI with trustworthy multi-modal complex intelligence in particular. 
        More specifically, my primary researches particularly focused on building infinite embodied AI foundation model for autonomous multimodal system
        agent, with data-driven neuro-computation innovation, in trustworthy human empathy and society value alignment of interdisciplinary research, towards AGI.
        
      </p>
      
      <h3 id="Research Interest">Research Interest & Selected Works</h3>
      <h5 <b> <span style="color:#9dbad1;">
        <div> PS: The details of the <b><span style="color:#9c0a1c;"> <tt>Confidential Projects</tt> </b> are NOT listed on the website (mianly works in the past two years), which included but not limited to: </div>
        <div>• <a href="https://www.youtube.com/watch?v=iBfjx9R3FP8" class="button">Demo</a> MSR-OpenAI Confidential Project [embodied action-multimodality pretraining, finetune, post-training, in-contextual learning with RL/IL/MPC/Decision-making] ; </div>
        <div>• <a href="https://www.sanctuary.ai/blog/sanctuary-ai-announces-microsoft-collaboration-to-accelerate-ai-development-for-general-purpose-robots" class="button">News</a> MSR-Sanctuary AI Confidential Project: </br>
               &nbsp;&nbsp;– Trustworthy Autonomous System [sim-to-real] for Spatial and Temporal Intelligence in Human-in-the-loop [mimic human behavior] with Human Value Alignment; </br>
               &nbsp;&nbsp;– Data-driven Neuro-computation HCI [cognition, bio-neuroscience, healthcare, etc.] in Dynamic Spatial Environment [humaniod robotics, bio-wearable machine (embodied cognition), aerospace navigation, etc.]. </div>
        </b> 
      </h5>
        <p>
          Broadly, my work focuses on embodied AI with lagre multi-modal foundation models generation, and how to inform machines about the world which interact with humans. 
          The research areas include infinite embodied foundation model generation for AI agents (pre-training, in-context learning, post-training with RL/IL/MPC/GAN/Decision-making), 
          and neural-symbolic interpretable representation for knowledge inference reasoning with AI agent acquisition and prediction of interdisciplinary research. 
          The recent topics of work include:
          <p>
            
          <b> <span style="color:#9dbad1;"><b> ♢ Foundation Models: Generalist Embodied Foundation Model with Trustworthy Multimodal Agent Intelligence </b></b>
            <div> <b>• Large Embodied Foundation Model [LLM/VLM/AVL(action-vision-languge) model] for pertraining, finetune, post-training, in-context learning : </b> Large embodied foundation models for flexible AI agent in Robotics, Spatial 3D/VR/AR/Gaming/mix-reality, 
          and Embodied Healthcare/Neuroscience/Cognition.
              <a href="https://arxiv.org/pdf/2402.05929">[paper1]</a> <a href="https://arxiv.org/abs/2403.00833">[paper2]</a> <a href="https://arxiv.org/pdf/2401.03568">[paper3]</a> <a href="https://agentfoundationmodel.github.io" class="button">Demo</a> <br />
              &nbsp;&nbsp;– parts models has been shipped to MS Product team, with Open AI. 
              <div> &nbsp;&nbsp;i) Robotics [manupulation, navigation, gesture, grasp, locomotion, teleoperation, execution];</div> 
              <div> &nbsp;&nbsp;ii) Spatial and Temporal intelligence for 2D/3D/VR/AR/Gaming/Mix-reality [simulation and real world models and agents]; </div>
              <div> &nbsp;&nbsp;iii) AVL/VLM/LLM for genelist vision-lanaguge task [generactive AI, video-audio-language, visual-captioning, VQA, 2D/3D environment generation/editing, classification tasks, etc.; </div>
              <div> &nbsp;&nbsp;iv) Embodied human-in-the-loop foudation modeling with neural-symbolic computation，healthcare, congnition in dynamic latent space. </div>
              </div>
            <div> <b>• Trustworthy Interactive Multi-agent Embodied System </b>with Optimization in Model Predictive Control (MPC) for Sim2Real dynamic multi-modality system 
              (simulation models to real robotics/VR/gaming/3D transferring); 
              <a href="https://arxiv.org/pdf/2309.09971">[paper]</a>  <a href="https://mindagent.github.io" class="button">Demo</a> <br />              
              &nbsp;&nbsp; We present <tt>MindAgent</tt>, an infrastructure for emergent HCI Spatial-Intelligence system, 
           enables<span style="color:#f1f0f1;"> <b>multi-agents optimization</b></span> and <span style="color:#f1f0f1;"> <b>human-agent interaction</b></span>.<br />
              &nbsp;&nbsp;– Proceedings of NAACL 2024. </div>
            <div> <b>• Generactive AI </b>with Action-Vision-Language Model (AVL), vision-lanaguage model (VLM), and large-language-model (LLM); 
              <b>Vision-Language Pre-training:</b> Large multimodality pre-training model;
              <a href="https://arxiv.org/abs/2305.00970">[paper1]</a> <a href="https://arxiv.org/abs/2205.09256">[paper2]</a><br />
              &nbsp;&nbsp;– the model has been shipped to MS Product team, with Open AI.<br />
              &nbsp;&nbsp;– Proceedings of Transactions on Machine Learning Research (T-MLR). 2023
            </div>
            <div> <b>• Embodied Chatbot for Mimicking Human Behavior </b>with Human Empathy and Society Value Alignment: NICE: Neural Image Commenting with Empathy. 
              Embodied dataset and benchmark generation.; <a href="https://nicedataset.github.io">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings of NAACL 2021.</div>
            <div> <b>• Embodied AI Agent Navigation with RL/IL/MPC:</b> Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation for robotics agent; 
             <a href="https://arxiv.org/pdf/1811.10092">[paper]</a> 
             <a href="https://www.microsoft.com/en-us/research/video/reinforced-cross-modal-matching-and-self-supervised-imitation-learning-for-vision-language-navigation/" class="button">Demo Link</a> <br />
              &nbsp;&nbsp;– Proceedings of CVPR 2019.  <span style="color:#9c0a1c;"><b> Best Student Paper Award.</b> </div> 
            <div> <b>• Augmented AVL/LLM/VLM with logical and knowledge inference reasoning interpratibility </b>for Huamn-in-the-loop Interaction. 
              <a href="https://aclanthology.org/2023.findings-acl.111.pdf">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings of ACL 2023. </div>
          </p>
              
          <p>
            <b> <span style="color:#9dbad1;"><b> ♢ Algorithms: Embodied AI with Reinforcement Learning (RL), Imitation Learning (IL), Model Predictive Control (MPC), 
              Generative Adversarial Network (GAN), and Decision-Making </b></b>
            <div> <b>• Embodied Autonomous Foundation Model for Generative AI with RL/IL/MPC</b>. <b>Generative AI (GenAI):</b> dynamic interaction model generation  
              in multimodality via emergent abilities with reinforcement learning and imitation learning.
              <a href="https://arxiv.org/abs/2305.00970">[paper1]</a> <a href="https://augmented-reality-knowledge.github.io" class="button">Demo</a>
              <a href="https://www.microsoft.com/en-us/research/uploads/prod/2021/10/kb_vlp_ICML2021.pdf">[paper2]</a> <a href="https://icml21ssl.github.io/pages/files/kb-vlp-poster.pdf" class="button">Poster</a> <br />
              &nbsp;&nbsp;– the model has been shipped to MS Product team, with Open AI.<br />
              &nbsp;&nbsp;– Proceedings of ICML 2021.</div>
            <div> <b>• Generative AI for Human-in-the-loop intelligence of LLM/VLM with RL</b>; 
              <a href="https://arxiv.org/abs/1805.08191">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings of AAAI 2019. Oral.</div>
            <div> <b>• Embodied AI with RL/IL/MPC:</b> Vision-Language Navigation Policy Learning and Adaptation for simulation agent robotics; 
              <a href="https://ieeexplore.ieee.org/document/8986691">[paper]</a> <br />
              &nbsp;&nbsp;– IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI) 2020.</div>
            <div> <b>• Embodied Robotics for Generative AI in Multi-modality with RL/GAN:</b> Turbo Learning for CaptionBot and DrawingBot; 
              <a href="https://arxiv.org/abs/1805.08170">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings of NeurIPS 2018. Oral.</div>
            <div> <b>• Embodied Communication:</b> Martian–message broadcast via LED lights to heterogeneous smartphones; 
              <a href="https://dl.acm.org/doi/abs/10.1145/2973750.2985257">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings of ACM Mobicom. Poster. <span style="color:#9c0a1c;"> <b>Best Poster Paper Award.</b> </div>
                                                                                                                                                                                                                 
          </p>

      
          <p>                                                                                                        
          <b> <span style="color:#9dbad1;"><b> ♢ [Sim-to-Real] Autonomous System: Dynamic System Control of Spatial and Temporal Interactive Intelligence for Neural-symblic Computation & Neuroscience/Cognition/Healthcare/Bionic Representation on Embodied Inference Interpretability 
            with Complexes Interdisciplinary Innovation </b></b>
            <div> <b>• Embodied Dynamics Autonomous Systems</b>, which conducts research in modeling and control of distributed parameters systems, 
              with applications to Autonomous Navigation, Robotics Systems (action perdiction, manupulation, motion planning and coordination, aerospace vehicle, 
              state estimation and localization, etc.) ; <a href="https://www.sanctuary.ai/blog/sanctuary-ai-announces-microsoft-collaboration-to-accelerate-ai-development-for-general-purpose-robots" class="button">Demo</a><br /></div>
            <div> <b>• Infinite Dimensional Systems </b>for Humaniod Robotics, Aerospace Navigation, and Wearable Divice with Cognition and Bio-Neurocomputation (mimic human behavior, manupulation, and emotion) 
              on Complex Intelligence. <br />
              <a href="https://www.youtube.com/watch?v=iBfjx9R3FP8" class="button">Demo</a>
              <a href="https://arxiv.org/abs/2305.00970">[paper1]</a> <a href="https://arxiv.org/pdf/2402.05929">[paper2]</a> <a href="https://arxiv.org/abs/1711.10485">[paper3]</a> <a href="https://arxiv.org/abs/1902.10740">[paper4]</a> <br />
              &nbsp;&nbsp;– the model has been shipped to MS Product team, with Open AI.<br />
              &nbsp;&nbsp;– Proceedings of CVPR 2019. <br />
              &nbsp;&nbsp;– Proceedings of CVPR 2018. 
            </div> 
           <div> <b>• Embodied AI of Neural-symbolic Computational Representation of Neural Network Inference Interpretation</b> (Healthcare, NeuroScience, Cognition) for Mimicking Human Behavior with Human Empathy Alignment in the cutting-edge frontiers research.
            <br />
               <div> &nbsp;&nbsp;i) Tensor Product Representation of Inference Neural-Symbolic Foundation Model with Neuroscience Interpretability;<a href="https://arxiv.org/abs/1910.02339v2">[paper]</a> <br />
              &nbsp;&nbsp; – Proceedings of ICML 2020. <br />
              &nbsp;&nbsp; – Proceedings of NeurIPS workshop 2019. <span style="color:#9c0a1c;"> <b>Best Paper Award.</b></div>
              <div> &nbsp;&nbsp;ii) Neural-Symbolic of Inference Intelligence in Generative AI: Attentive Tensor Product Learning;<a href="https://arxiv.org/abs/1802.07089">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings of AAAI 2019 </div> 
              <div> &nbsp;&nbsp;iii) Neural-Symbolic Representation of Tensor Product Generative AI: Tensor Product Generation Networks for Deep NLP Modeling; 
              <a href="https://aclanthology.org/N18-1114/">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings of NAACL 2018. </div>
            </div>  
          </p>

          <p>
          <b> <span style="color:#9dbad1;"><b>♢ Trustworthy Data-driven Distributed Machine Learning and Multi-agent Optimization: Trustworthy Dataverse Interactive Learning and Multimedia Networking in Latent Space with Trustworthy Huamn Empathy and Society Value Alignment</b></b>
            <div> <b>• Knowledge Inference Reasoning Representation </b>for Large Foundation Models in Cognition Complex with Trustworthy Dataverse
              (mimic human behavior and inference-interpretability); 
              <br />
              &nbsp;&nbsp;– parts models has been shipped to MS Product team, with Open AI. 
              <div> &nbsp;&nbsp;i) Spatial AI: Retrieve What You Need: Post-Training for knowledge agent with RL;<a href="https://aclanthology.org/2024.tacl-1.14/">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings in Transactions of ACL (T-ACL), 2024 </div> 
              <div> &nbsp;&nbsp;ii) Localized Symbolic Knowledge Distillation for Visual Commonsense Models in Post-Training via ChatGPT; 
              <a href="https://arxiv.org/pdf/2312.04837">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings of NeurIPS 2023. </div>
              <div> &nbsp;&nbsp;iii) Logical Transformer for neurocumputation Neurocomputation Interpretability Representation;  <a href="https://aclanthology.org/2023.findings-acl.111.pdf">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings of ACL 2023. </div>
              <div> &nbsp;&nbsp;iv) KAT: A Knowledge Augmented Transformer for Vision-and-Language;  <a href="https://arxiv.org/pdf/2112.08614">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings of NAACL 2022. <span style="color:#9c0a1c;"> <b>No.1 State-of-The-Art (SoTA) Stage of Multimodality (OK-VQA) Leaderboard 2022.</b></div> 
              </div>
            <div> <b>• Embodied AI of Data-driven computational and evaluation representation in distributed system governance</b>
              of across-disciplinary Boundaries.  
              <div> &nbsp;&nbsp;i) “TIGEr: Text-to-Image Grounding for Image Caption Evaluation”; <a href="https://arxiv.org/abs/1909.02050">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings in EMNLP, 2019 </div> 
             <div> &nbsp;&nbsp;ii) “REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning”; <a href="https://arxiv.org/abs/1909.02217">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings in EMNLP, 2019 </div> 
            </div>
      <div> <b>• Embodied Computer Engineering in Trustworthy Multimedia Networking of Human-in-the-loop Distributed Machine Learning in Cutting-edge Frontiers Research. </b>
              <div> &nbsp;&nbsp;i) "Vision and Challenges for Knowledge Centric Networking”; <a href="https://ieeexplore.ieee.org/abstract/document/8685777">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings in IEEE Wireless Communications, 2018 </div> 
             <div> &nbsp;&nbsp;ii) “Trickle Irrigation: Congestion Relief for Communication with Network Coding”; <a href="https://ieeexplore.ieee.org/document/8373732">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings of IEEE Transactions on Vehicular Technology. 2017 </div> 
              <div> &nbsp;&nbsp;iii) "Just fun: A joint fountain coding and network coding approach to loss-tolerant information spreading”; <a href="https://mm.aueb.gr/research/mobihoc14/mobihoc/p83.pdf">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings in ACM MobiHoc, 2014 </div> 
              <div> &nbsp;&nbsp;iv) “Social networking reduces peak power consumption in smart grid”; <a href="http://staff.ustc.edu.cn/~xiangyangli/paper/Journal/SocialSmartGrid-TG.pdf">[paper]</a> <br />
              &nbsp;&nbsp;– Proceedings of IEEE Transactions on Smart Grid. 2014 </div> 
            </div>         
            
            </p>
         
        </p>

        <p> 
          During the past year, my research at MSR has centered around the theme of Infinite Embodied AI 
         for pre-trained large multimodal foundation models — an autonomous system that integrates action-vision-language models into interactive agnet with neurocomputation inference knowledge
          with trustworthy value alignment.
          I believe, by integrating embodied AI with automous system on spatail and temporal distribution information, we can achieve better yet interpretable, grounded and robust multi-modality intelligent agents for cross-modality and agnostic-reality.
          Beyond pushing state-of-the-art research, I am a strong proponent of efficient and reproducible research. We also helped to ship Embodied AI techniques to Microsoft products and to create real-world impact.
          I enjoy the process of bridging the gap between theory and practice, bringing theoretical results to practical applications. 
          I also enjoy coding, building real systems from scratch, and making them work simply, efficiently, and elegantly. 
        </p>
      
      <h3 id="Talks">Recent Talks</h3>
        <h4 id="Task1">My recent talks at academia & industry for the research and practical applications involving:</h4>

      <p>
          <tt>• Nov 2024:</tt> Talk on <a>Embodied AI with Spatial and Temporal Intelligence Alignment</a> at Google Research. 
          </p>
      <p>
          <tt>• Nov 2024:</tt> Talk on <a>Embodied Autonomous System for Robotics and Navigation</a> at Waymo Research.
          </p>

       <p>
          <tt>• Oct 2024:</tt> Talk on <a>Embodied AI with Multimodal Agent</a> at Cambridge Univeristy. 
          </p>

         <p>
          <tt>• Aug-Oct 2024:</tt> Talk on <a>Embodied AI </a> at Harvard Univeristy.
          <p style="text-align: justify; font-size: 1.0em;">&nbsp;&nbsp;&nbsp; - Talk on <tt>Embodied AI</tt>, with 
           enables<span style="color:#f1f0f1;"> <b>Human Empathy</b></span> and <span style="color:#f1f0f1;">
           <b>Socity Value alignment</b></span>.
          </p>
          <p style="text-align: justify; font-size: 1.0em;">&nbsp;&nbsp;&nbsp; - Talk on <tt>AI Agnet</tt>, for
           enables<span style="color:#f1f0f1;"> <b>human-in-the-loop intelligence innovation</b></span> with <span style="color:#f1f0f1;">
           <b>interdisciplinary research</b></span>.
          </p>      
          </p>

        <p>
          <tt>• Aug 2024:</tt> Talk on <a>Dynamic Interaction Generative AI (GenAI) in Robotics and Multimodality </a> at Princeton University.
        </p>
      
        <p>
          <tt>• Jul 2024:</tt> Talk on <a>Infinite Embodied Spatial Intelligence in Real and Virtual World </a> at Stanford University.            
        </p> 

        <p>
          <tt>• May 2024:</tt> Talk on <a>"Embodied Per-training Foudation Model"</a> at Microsoft Research AI Frontiers.       
         </p> 

         <p>
          <tt>• Mar 2024:</tt> Talk on <a>"Embodied Agent Foudation Model"</a> at Project Green at Microsoft.       
         </p> 

        <p>
          <tt>• Sep 2023:</tt> Talk on <a>"Autonomous Multi-model Agent Interaction System"</a> at the Research Summit at Microsoft Research: 
          <div>          
          &nbsp;&nbsp;&nbsp;- Talk at MS Mesh team; the model has been shipped to MS office teams.
          </div> 
          <div>          
          &nbsp;&nbsp;&nbsp;- Talk at MS Gaming team; Robotics Team.
          </div>
          <div>          
          &nbsp;&nbsp;&nbsp;- Talk at MS Turing Team and Bing search; the model has been shipped to MS Azure teams.
          </div>        
        </p> 
      
        <p>
          <tt>• Oct 2022:</tt> Talk on <a>Panel for "Large-scale Embodied AI "</a>at Microsoft Research Summit.
        </p> 
      
          <!--p>
          <b>• Embodied Action Foundation Model:</b> Large embodied foundation models for flexible AI agent infrastructure with vision-language. 
          "An Interactive Agent Foundation Model" of large action-vision-language per-training for embodied AI in Robotics, Gaming/mix-reality, 
          and Embodied Healthcare. The model developed in collaboration with MS Product Robotics team, Gaming Team, Turing team, and Stanford.
          <a href="https://agentfoundationmodel.github.io/pdfs/paper.pdf" class="button">Link</a>
          <a href="https://agentfoundationmodel.github.io" class="button">Demo Link</a>
           
          </p>-->

         <!--p>
          <b>• Multi-agent Infrastructure with In-contextual Learning:</b> "MindAgent: multi-agent infrastructure with GPT-4 (V) ison 
          for infinite spatial intelligence in real and virtual world", in collaboration with MS X-box team, and shipped to MS Gaming team; 
           <a href="https://mindagent.github.io" class="button">Demo Link</a>
      
          <p style="text-align: justify; font-size: 1.0em;">We present <tt>MindAgent</tt>, an infrastructure for emergent gaming interaction, 
           enables<span style="color:#f1f0f1;"> <b>multi agents collaboration</b></span> and <span style="color:#f1f0f1;">
           <b>human-agent collaboration</b></span>.
          </p>      
          </p>-->

        <!--p>
          <b>• Post-Training for Generative AI:</b> dynamic interaction model generation (GenAI) in multimodality via emergent abilities. "ArK: 
          Augmented reality with emergent infrastructure". Post-training using reinforcement and imitation learning for generative AI (GPT, Dall-E), 
          in collaboration with MS Mesh team; the model has been shipped to MS office teams;
                 
        <a href="https://arxiv.org/pdf/2305.00970" class="button">Link</a>
               
        </p>-->
      
        <!--p>
          <b>• Applications for Human-Machine Interaction:</b> Embodied Multimodal Navigation with RL & IL. "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning 
          for Vision-Language Navigation" for simulation agent robotics, in collaboration with MS Cognition Services team;
                  
        <a href="https://arxiv.org/pdf/1811.10092" class="button">Link</a>
         <a href="https://www.microsoft.com/en-us/research/video/reinforced-cross-modal-matching-and-self-supervised-imitation-learning-for-vision-language-navigation/" class="button">Demo Link</a>                
        </p>-->  

         <!--p>
          <b>• Vision-Language Pre-training:</b> "Training Vision-Language Transformers from Captions", 
           Vision-language pre-training model generation in collaboration with Turing team and Cognition Services;
                   
         <a href="https://arxiv.org/abs/2205.09256" class="button">Link</a>
                 
         </p>--> 

        <!--p>
          <b>• Knowledge Inference and Representation in Multi-modality:</b> Knowledge-Inference Reasoning Transformer: 
          <div>          
          i) "KAT: A Knowledge Augmented Transformer for Vision-and-Language", <a href="https://arxiv.org/abs/2112.08614" class="button">Link</a>
          </div> 
          <div>          
          ii) "Retrieve What You Need: knowledge-LLM agent with GPT", <a href="https://aclanthology.org/2024.tacl-1.14/" class="button">Link</a>
          </div>
          <div>          
          iii) "Logical Transformer", which collaborated with MS Turing Team and Bing search. 
          <a href="https://aclanthology.org/2023.findings-acl.111.pdf" class="button">Link</a>
          </div>        
        </p>--> 
              
        
        <h3 id="Publications">Selected Publications   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   【*Equal Contribution. ▶Project Lead.】   </h3>  
        <h4 id="Task1">(please refer to <a href="https://scholar.google.com/citations?user=U7Mmyc8AAAAJ&hl=en" class="button">Google Scholar</a> for the full publications)</h4>          
           
        <p>
          <b>■ An Interactive Agent Foundation Model</b> for embodied interaction in Robot, Gaming, and Healthcare.
        <div>
          Z. Durante*, R. Gong*, R. Taori, Y. Noda, P. Tang, 
          E. Adeli, S. Kowshika Lakshmikanth, K. Schulman, A. Milstein, D. Terzopoulos, A. Famoti, 
          N. Kuno, A. Llorens, H. Vo, K. Ikeuchi, L. Fei-Fei, J. Gao, N. Wake*▶, <b><u>Q. Huang*▶</u></b>.  <br /> 
          *Equal Contribution. ▶Project Lead.
        </div>          
          arXiv:2402.05929, May 2024.
          <a href="https://arxiv.org/pdf/2402.05929">[paper]</a>
          <a href="https://agentfoundationmodel.github.io">[webpage]</a>           
         </p>

        <p>
          <b>■ Agent AI Towards a Holistic Intelligence.</b>
        <div>
          <b><u>Q. Huang</u></b>, N. Wake, Z. Durante, R. Gong, R. Taori, Y. Noda, D. Terzopoulos, 
          N. Kuno, A. Famoti, A. Llorens, J. Langford, H. Vo, L. Fei-Fei, K. Ikeuchi, J. Gao.        
        </div>          
          arXiv:2403.00833, May 2024.          
          <a href="https://arxiv.org/abs/2403.00833">[paper]</a>        
    </p>

        <p> 
          <b>■ MindAgent: Emergent Gaming Interaction.</b>
        <div>R. Gong*, <b><u>Q. Huang*▶</u></b>, X. Ma*, H. Vo, Z. Durante, Y. Noda, Z. Zheng, 
          S. Zhu, D. Terzopoulos, L. Fei-Fei, J. Gao.  
          *Equal Contribution. ▶Project Lead.        
        </div>          
        Proceedings of NAACL 2024, Jun. 2024.          
          <a href="https://arxiv.org/pdf/2309.09971">[paper]</a>
          <a href="https://mindagent.github.io">[webpage]</a>         
    </p>  

    <p> 
          <b>■ Agent AI: Surveying the Horizons of Multimodal Interaction.</b>
        <div>Z. Durante*, <b><u>Q. Huang*▶</u></b>, N. Wake*, R. Gong, J. Park, B. Sarkar, 
          R. Taori, Y. Noda, D. Terzopoulos, Y. Choi, K. Ikeuchi, H. Vo, L. Fei-Fei, J. Gao.   *Equal Contribution. ▶Project Lead.</div>          
        arXiv:2401.03568, Jan 2024.
          <a href="https://arxiv.org/abs/2401.03568">[paper]</a>         
    </p> 

  <p> 
          <b>■ Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering.</b>
        <div>D. Wang, <b><u>Q. Huang</u></b>, M. Jackson, J. Gao</div>          
        Proceedings of Transactions of ACL (TACL) 2024. Apr. 2024
      <a href="https://aclanthology.org/2024.tacl-1.14/">[paper]</a>         
    </p>       

    <p> 
          <b>■ ArK: Augmented Reality with Knowledge Interactive Emergent Ability. (Generative AI)   </b>
        <div> <b><u>Q. Huang</u></b>, J. Park, A. Gupta, P. Bennett, R. Gong, S. Som, B. Peng, O. Mohammed, C. Pal, Y. Choi, J. Gao.         
        </div>          
        arXiv:2305.00970. Shipped the model in Microsoft Teams for product, collobarated with Microsoft Mesh team and Turing team. Jun. 2023
      <a href="https://arxiv.org/abs/2305.00970">[paper]</a> 
      <a href="https://augmented-reality-knowledge.github.io">[webpage]</a>
      <a href="https://www.microsoft.com/en-us/research/project/mixed-reality/">[product page]</a> 
    </p>

      

    <p> 
          <b>■ Localized Symbolic Knowledge Distillation: Infinite Knowledge Distillation in Multi-modality (Knowledge-auto-GPT)</b>
        <div>J. Park, J. Hessel, K. Chandu, P. Liang, X. Lu, P. West, Y. Yu, <b><u>Q. Huang</u></b>, 
          J. Gao, A. Farhadi, Y. Choi
        </div>          
        Proceedings of NeurIPS 2023. Dec.2023
          <a href="https://arxiv.org/abs/2312.04837">[paper]</a>        
    </p>  

        <p> 
          <b>■ Logical Transformers: Infusing Logical Structures into Pre-Trained Language Models</b> 
        <div>B. Wang, <b><u>Q. Huang▶</u></b>, B. Deb, A. Halfaker, L. Shao, D. McDuff, A. Awadallah, D. Radev, J. Gao</div>          
          Proceedings of ACL 2023. Jul.2023
            <a href="https://aclanthology.org/2023.findings-acl.111.pdf">[paper]</a>   
      </p> 

        <p> 
          <b>■ Training Vision-Language Transformers from Captions (Vision-Lanaguage Pertraining)</b> 
        <div>L. Gui, Y. Chang, <b><u>Q. Huang▶</u></b>, S. Som, A. Hauptmann, J. Gao, Y. Bisk.</div>          
          Proceedings of Transactions on Machine Learning Research. 2023
      <a href="https://arxiv.org/abs/2205.09256">[paper]</a>   
      </p> 

      
      
    <p> 
          <b>■ KAT: A Knowledge Augmented Transformer for Vision-and-Language.</b> <span style="color:#9c0a1c;"><b> No.1 State-of-The-Art (SoTA) Stage of Multimodality (OK-VQA) Leaderboard (2022).</b>
        <div>L. Gui, B. Wang, <b><u>Q. Huang▶</u></b>, A. Hauptmann, Y. Bisk, J. Gao
        </div>          
        Proceedings of NAACL 2022, Jun. 2022.  
          <a href="https://arxiv.org/abs/2112.08614">[paper]</a>   
      </p> 

    <p> 
          <b>■ NICE: Neural Image Commenting with Empathy.</b> <span style="color:#9c0a1c;"><b> New Large-scale Per-training Dataset, Modales and Benchmack of Embodied Multimodality with Human Value Alignment.</b>
        <div>K. Chen, <b><u>Q. Huang▶</u></b>, D. McDuff, X. Gao, H. Palangi, J. Wang, K. Forbus, J. Gao
        </div>          
        Proceedings of EMNLP 2021, Jan. 2021.  
          <a href="https://aclanthology.org/2021.findings-emnlp.380.pdf">[paper]</a>   
      </p> 


   <p> 
          <b>■ Vision-Language Navigation Policy Learning and Adaptation.</b> 
        <div>X. Wang, <b><u>Q. Huang▶</u></b>, A. Celikyilmaz, J. Gao, D. Shen, Y. Wang, W. Wang, L. Zhang
        </div> 
        IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI). Jun. 2020. <a href="https://ieeexplore.ieee.org/document/8986691">[paper]</a>    
       </p>      
 
       <p> 
          <b>■ Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation.</b> 
            <span style="color:#9c0a1c;"><b> Best Student Paper Award.</b>
        <div>X. Wang, <b><u>Q. Huang▶</u></b>, A. Celikyilmaz, J. Gao, D. Shen, Y. Wang, W. Wang, L. Zhang
        </div> 
        Proceedings of CVPR 2019. Jun. 2019. <a href="https://arxiv.org/abs/1811.10092">[paper]</a>    
       </p>  


       <p> 
          <b>■ Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations.</b>
        <div> K. Chen, <b><u>Q. Huang▶</u></b>, H. Palangi, P. Smolensky, K. Forbus, J. Gao
        </div>          
         Proceedings of ICML 2020. Feb.2020          
      <a href="https://arxiv.org/abs/1910.02339">[paper]</a>   
      </p>  

      <p> 
          <b>■ TP-N2F: Tensor Product Representation for Natural To Formal Language Generation.</b> <span style="color:#9c0a1c;"><b>Best Paper Award.</b>
        <div>K. Chen, <b><u>Q. Huang▶</u></b>, H. Palangi, P. Smolensky, K. Forbus, J. Gao. </div>          
          Proceedings of NeurIPS workshop 2019. Dec.2019.  
          <a href="https://arxiv.org/abs/1910.02339v2">[paper]</a>   
      </p>  

       <p> 
          <b>■ TIGEr: Text-to-Image Grounding for Image Caption Evaluation.</b>
        <div>M. Jiang, <b><u>Q. Huang▶</u></b>, L. Zhang, X. Wang, P. Zhang, Z. Gan, J. Diesner, J. Gao
        </div>          
        Proceedings of EMNLP 2021, Apr. 2019.  
          <a href="https://arxiv.org/abs/2112.08614">[paper]</a>   
      </p> 

      <p> 
          <b>■ REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning.</b>
        <div>M. Jiang, J. Hu, <b><u>Q. Huang</u></b>, L. Zhang, J. Diesner, J. Gao
        </div>          
        Proceedings of EMNLP 2021, Apr. 2019.  
          <a href="https://arxiv.org/abs/2112.08614">[paper]</a>   
    </p> 
            

       <p> 
          <b>■ Object-driven Text-to-Image Synthesis via Adversarial Training.</b>
        <div>W. Li, P. Zhang, L. Zhang, <b><u>Q. Huang</u></b>, X. He, S. Lyu, J. Gao
        </div>          
        Proceedings of CVPR 2019, Jun. 2019.  
          <a href="https://arxiv.org/abs/1902.10740">[paper]</a>   
    </p> 


       <p> 
          <b>■ Attentive Tensor Product Learning.</b> 
        <div> <b><u>Q. Huang</u></b>, L. Deng, O. Wu, C. Liu, X.He</div>          
          Proceedings of AAAI 2019. Feb. 2019
          <a href="https://arxiv.org/abs/1802.07089">[paper]</a>   
      </p> 

       <p> 
          <b>■ Turbo Learning for CaptionBot and DrawingBot.</b> 
        <div> <b><u>Q. Huang</u></b>, P. Zhang, D. Wu, L. Zhang.</div>          
          Proceedings of NeurIPS 2018. Dec.2018.  
          <a href="https://arxiv.org/abs/1805.08170">[paper]</a>   
      </p>  

        <p> 
          <b>■ Hierarchically Structured Reinforcement Learning for Topically Coherent Visual (video) Story Generation.</b> 
        <div> <b><u>Q. Huang*</u></b>, Z. Gan*, A. Celikyilmaz, L. Li, D. Wu, X. He. *Equal contribution. </div>          
          Proceedings of AAAI 2019. Feb.2019          
       <a href="https://arxiv.org/abs/1805.08191">[paper]</a>   
      </p> 

         <p> 
          <b>■ Attentive Tensor Product Learning.</b> 
        <div> <b><u>Q. Huang</u></b>, L. Deng, O. Wu, C. Liu, X. He.</div>          
          Proceedings of AAAI 2019. Feb.2019          
      <a href="https://arxiv.org/abs/1802.07089">[paper]</a>   
      </p>


         <p> 
          <b>■ AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks.</b> 
        <div>T. Xu, P. Zhang▶, <b><u>Q. Huang▶</u></b>, H. Zhang, Z. Gan, X. Huang, X. He.</div>          
          Proceedings of CVPR 2018. Jun.2018          
      <a href="https://arxiv.org/abs/1711.10485">[paper]</a>   
      </p> 

       <p> 
          <b>■ Tensor Product Generation Networks for Deep NLP Modeling.</b> 
        <div> <b><u>Q. Huang</u></b>, P. Smolensky, X. He, L. Deng, D. Wu, Long paper.</div>          
          Proceedings of NAACL 2018, ACL–Association for Computational Linguistics, New Orleans, Louisiana, US. Jun.1- Jun.6. 2018.
     <a href="https://aclanthology.org/N18-1114/">[paper]</a>   
      </p> 


          <p> 
          <b>■ Martian--message broadcast via LED lights to heterogeneous smartphones: poster.</b> <span style="color:#9c0a1c;"><b>Best Poster Paper Award.</b>        
            <div> H. Du, J. Han, <b><u>Q. Huang</u></b>, X. Jian, C. Bo, Y. Wang, X.-Y. Li.</div>          
          Proceedings of the 22nd ACM MobiCom, 417-418. Feb. 2016
         <a href="https://dl.acm.org/doi/abs/10.1145/2973750.2985257">[paper]</a>   
          </p> 

           <p> 
          <b>■ Job Scheduling Under Differential Pricing: Hardness and Approximation Algorithms.</b>    
            <div> <b><u>Q. Huang</u></b>, J. Zhao, H. Du, J. Hou, X.-Y. Li.</div>          
          Proceedings of the IEEE WASA, 417-418. Dec. 2016
         <a href="https://link.springer.com/chapter/10.1007/978-3-319-60033-8_55">[paper]</a>   
          </p>
      

         <p> 
          <b>■ Just fun: A joint fountain coding and network coding approach to loss-tolerant information spreading. </b>        
            <div> <b><u>Q. Huang</u></b>, K. Sun, X. Li, O. Wu.</div>          
          Proceedings of the ACM MobiHoc. Aug. 2014
         <a href="https://mm.aueb.gr/research/mobihoc14/mobihoc/p83.pdf">[paper]</a>   
          </p> 


         <p> 
          <b>■ Social networking reduces peak power consumption in smart grid.</b>       
            <div> <b><u>Q. Huang</u></b>, X. Li, J. Zhao, O. Wu, X.-Y. Li.</div>          
          Proceedings of IEEE Transactions on Smart Grid. Dec. 2014
         <a href="http://staff.ustc.edu.cn/~xiangyangli/paper/Journal/SocialSmartGrid-TG.pdf">[paper]</a>   
          </p> 

          <p> 
          <b>■ Smoothing the energy consumption: Peak demand reduction in smart grid.</b>       
            <div> S. Tang, <b><u>Q. Huang</u></b>, X. Li, O. Wu.</div>          
          Proceedings of IEEE INFOCOM. Apr. 2013
         <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ef3ad97b671c4b94338cc7d37d99b24ce0522f14">[paper]</a>   
          </p> 

          <p> 
          <b>■ Flatten a Curved Space by Kernel: From Einstein to Euclid.</b>       
            <div> <b><u>Q. Huang</u></b>, O. Wu.</div>          
          Proceedings of IEEE Signal Processing Magazine [Impact Factor: 9.4]. Sep. 2013
         <a href="http://www.wu.ece.ufl.edu/mypapers/Kernel-basedLearning_technicalReport.pdf">[paper]</a>   
          </p> 

          <p> 
          <b>■ SilentSense: silent user identification via touch and movement behavioral biometrics.</b>       
            <div> C. Bo, L. Zhang, X.Li, <b><u>Q. Huang</u></b>, Y. Wang.</div>          
          Proceedings of IEEE Mobicom. Sep. 2013
         <a href="https://web.archive.org/web/20170706064256id_/http://webpages.uncc.edu/ywang32/research/mobicomPOS06-bo.pdf">[paper]</a>   
          </p> 


      


      
      
      
      <h3 id="ToolDownload">Data Collection and Analyzation Tools Download</h3>
        <p>
        I am also working on the trustworthy embodied data-driven alignment for multimodal data collection, safety analysis, and related Benchmark generation which collaborated with MS Turing team, Robotics team, X-box
        team, University of Washington, Stanford University, Sanctuary AI, and Open AI.
        </p>
        <p>
          <b>Data Collection and Analyzation Collection: </b> For our released <br> 
          New Dataset and Banchmark of Embodied AI with Human Value Aligment: <a href="https://nicedataset.github.io">“NICE” dataset and banchmark: embodied data-driven multimodality for mimic human action, enmotion, with human empathy and sccity value alignment.</a> <br /> 
          New Dataset, Banchmark, and HCI System of Multi-Agent MPC and Optimization: <a href="https://github.com/mindagent/mindagent">“Cuisine world” dataset, benchmark, and infrastructure: embodied multi-agent data-driven for human-machine-interaction in Spatial and Temporal Intelligence</a> etc. <br /> 
          You can find the codes for data collection below: 
        </p>
        <p><a href="https://github.com/NICEdataset/DataCollectionTool" class="button">Data Collection Tools</a></p>
       
      
      <h3 id="EULA">User License Agreement (EULA) for Using Our Dataset</h3>
        <p>
          You should complete an end user license agreement (EULA) before accessing the dataset/benchmark/infractrcuture. 
          The EULA will be posted soon once the data releasing process is finished. 
        </p>
        <a href="https://github.com/nicedataset/NICEdataset.github.io/blob/master/End%20User%20License%20Agreement.pdf" class="button">Download EULA</a>
      
      
      <h3 id="Fun">Fun</h3>
        <p>
        <p>Ballet, Violin, Tennis, Hiking</p>
        </p>
        
        
    </div>
  </body>

</html>
