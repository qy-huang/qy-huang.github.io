<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Qiuyuan Huang</title>
    <link rel="stylesheet" href="style.css">
    <!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<script>
function UpdateTaskIllustration() {
  var task = document.getElementById("illu-task").value;
  console.log("illu", task)
  var img = document.getElementById("task-illustration");
  img.src = "assets/" + "illu-" + task + ".png"
}

function UpdateTaskDemonstration() {
  var task = document.getElementById("demo-task").value;
  console.log("demo", task)
  var video = document.getElementById("task-demonstration");
  video.src = "assets/" + "demo-" + task + ".mp4"
  video.playbackRate = 1.75;
  video.play();
}
</script>
  </head>
  
  <body>
    <div class="topnav">
      <div class="topnav-left">
        <img src="Q5.png"  width="115" height="115" alt="Microsoft Research">  
        <a href="index.html">Qiuyuan (Enno) Huang</a>
        <a href="mailto:theidfree@gmail.com"><class="button style="font-size:18px"><i class="fa fa-envelope"></i> Email</button></a>
        <a href="https://scholar.google.com/citations?user=U7Mmyc8AAAAJ&hl=en"><class="button style="font-size:18px"><i class="fa fa-book"></i> Google Scholar</button></a> 
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a>
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a> 
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a> 
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a> 
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a> 
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a> 
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a> 
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a>
        <a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </a>
      </div>
    </div>    <div class="header">
      <p></p>
    </div>
    <div class="sidenav">
      <a href="#Research Interest">Research Interest</a>
      <!--a href="#Talks">Recent Talks</a>-->
      <a href="#Publications">Publications</a>
      <a href="#ToolDownload">Trustworthy Data-Driven Tools</a>
      <a href="#EULA">Dataverse User License </a>
      <a href="#Fun">Fun</a>
    </div>
    <div class="main">
      <p>I am a Principal Research Scientist at Microsoft Research (MSR), Redmond, WA. 
        My current research interests are in deep learning in general, and embodied AI with trustworthy multi-modal complex intelligence in particular. 
        More specifically, my primary researches particularly focused on building infinite embodied AI foundation model for autonomous HCI system [interactive system], multimodal agent [optimization & infrastructure], 
        general purpose robotics [sim-to-real], with data-driven neuro-computation representation and generative AI innovation, in trustworthy human empathy and society value alignment of interdisciplinary research, towards AGI.
        
      </p>
      
      <h3 id="Research Interest">Research Interest & Selected Works</h3>
      <h5 <b> <span style="color:#dbd7d5;">
        <div> <b><span style="color:#9dbad1;">[ Product-Driven Research Projects ] </b><br /> 
              <span style="font-weight: 400"> My product commitment works in the past two years, which included but not limited to: (the details of the <b><span style="color:#9dbad1;"> <tt>Product Projects</tt> </b> are NOT listed on the website) </div>
        <div>■ <b><span style="color:#9c0a1c;"><i class="fa fa-windows" aria-hidden="true"></i></b> <span style="font-weight: 400"> <b><span style="color:#9dbad1;">MSR-OpenAI Confidential Project:</b> 
          Large Embodied Foundation Models [pretraining, finetune, in-context learning, post-training with RL/IL/GAN/MPC/Decision-making] for Embodied Multimodal Agent. 
          <a href="https://www.youtube.com/watch?v=iBfjx9R3FP8" class="button">Demo</a> <a href="https://www.youtube.com/watch?v=qw5GQQThbSY" class="button">Evaluation Podcast for Our Paper</a></div>     
        <div>■ <b><span style="color:#9c0a1c;"><i class="fa fa-windows" aria-hidden="true"></i></b> <span style="font-weight: 400"> <b><span style="color:#9dbad1;">MSR-Sanctuary AI Confidential Project: </b>
               Trustworthy Autonomous Interactive System for Spatial and Temporal Intelligence in Human-in-the-loop [mimic human logic, cognition, and behavior] with trustworthy human empathy and society value alignment. <a href="https://www.sanctuary.ai/blog/sanctuary-ai-announces-microsoft-collaboration-to-accelerate-ai-development-for-general-purpose-robots" class="button">News</a> <a href="https://www.sanctuary.ai/blog/sanctuary-ai-unveils-the-next-generation-of-ai-robotics" class="button"> General Purpose Robots </a> <br/>
               &nbsp;&nbsp;&nbsp;– <tt>[sim-to-real HCI system]</tt> Humaniod robotics, general purpose robotics, bio-wearable machine (embodied cognition/healthcare), affective computing, spatial-3D/AR/VR/mix-reality world model, autonomous manufacturing system, aerospace navigation, etc. </div>
        <div>■ <b><span style="color:#9c0a1c;"><i class="fa fa-windows" aria-hidden="true"></i></b> <span style="font-weight: 400"><b><span style="color:#9dbad1;">MSR-Turing Confidential Project:</b> Data-driven Neuro-computation HCI for LFM Inference Interpretability in Dynamic Latent Spatial Environment with Interdisciplinary Research. <br/>
               &nbsp;&nbsp;&nbsp;– Neural-symbolic computational representation [cognition, bio-neuroscience, healthcare, affective-computing, logic-reasoning, etc.] for the interpretability of large foundation model. <br/>
               &nbsp;&nbsp;&nbsp;– Dataverse knowledge-logic inference reasoning for Genractive AI on healthcare, conganition, neuroscience, affective-computing and quantitative-computation. <br /> 
        <br /> 
        </div>
       
      </b> 
      </h5>
        <p>
          <b> <span style="color:#9dbad1;">[ Research-Driven Projects ] </b><br /> 
          Broadly, my research work focuses on <b><span style="color:#9dbad1;">Embodied AI</b> for <b><span style="color:#9dbad1;">trustworthy autonomous HCI system </b> with <b><span style="color:#9dbad1;">data-driven neuro-computation interpretability and ethics value alignment </b> of interdisciplinary research. 
          The research areas include: <br />
          • <b><span style="color:#9c0a1c;"><i class="fa fa-pagelines" aria-hidden="true"></i></b> 
          <b><span style="color:#9dbad1;">Infinite embodied foundation model </b>(pre-training, finetune, in-context learning, post-training with RL / IL / MPC / GAN / Decision-making) of <b><span style="color:#9dbad1;">multimodal (action-vision-language-knowledge) agents for autonomous HCI system </b> <br />
          • <b><span style="color:#9c0a1c;"><i class="fa fa-pagelines" aria-hidden="true"></i></b> <b><span style="color:#9dbad1;">Data-driven neuro-computation (neural-symbolic) interpretable representation with spatial & temporal knowledge inference reasoning </b>(mimic human cognition and behavior) <br />
          • <b><span style="color:#9c0a1c;"><i class="fa fa-pagelines" aria-hidden="true"></i></b> <b><span style="color:#9dbad1;">Generative AI and </b>in <b><span style="color:#9dbad1;">trustworthy human empathy and ethics society value alignment </b><br /> 
          The selected works as follows:
          <p>
            
          <b> <span style="color:#9dbad1;"><b> ♢ Foundation Models: Generalist Embodied Foundation Model with Trustworthy Multimodal Agent Intelligence </b></b>
            <div> <b>■ Large Embodied Foundation Model [LLM / VLM / AVL(action-vision-languge) model] for pertraining, finetune, post-training, in-context learning : </b><br /> Large embodied foundation models for flexible AI agent in Robotics, Spatial 3D/VR/AR/Gaming/mix-reality, 
          and Embodied Healthcare/Neuroscience/Cognition. <br />
              <a href="https://arxiv.org/pdf/2402.05929">[paper1]</a> <a href="https://arxiv.org/abs/2403.00833">[paper2]</a> 
              <a href="https://arxiv.org/pdf/2401.03568">[paper3]</a> <a href="https://agentfoundationmodel.github.io" class="button">Demo</a> 
              <a href="https://www.youtube.com/watch?v=7Z4r7VkiRrk" class="button">Paper Podcast</a>
               <br /> 
              &nbsp;&nbsp;&nbsp;– Partial models has been shipped to MS Product team, with Open AI. <br />
              <div> &nbsp;&nbsp;i) Robotics [manupulation, navigation, gesture, grasp, locomotion, teleoperation, execution].</div> 
              <div> &nbsp;&nbsp;ii) Spatial and Temporal intelligence for 2D/3D/VR/AR/Gaming/Mix-reality [simulation and real world models and agents]. </div>
              <div> &nbsp;&nbsp;iii) AVL/VLM/LLM for genelist vision-lanaguge task [generactive AI, video-audio-language, visual-captioning, VQA, 2D/3D environment generation/editing, classification tasks, etc. </div>
              <div> &nbsp;&nbsp;iv) Embodied human-in-the-loop foudation modeling with neural-symbolic computation，healthcare, congnition in dynamic latent space. </div>
              </div>
            <div> <b>■ Trustworthy Interactive Multi-agent Embodied HCI System </b>with Optimization in Model Predictive Control (MPC) for Sim2Real dynamic multi-modality system 
              (simulation models to real robotics/VR/gaming/3D transferring); 
              <a href="https://aclanthology.org/2024.findings-naacl.200.pdf">[paper]</a>  <a href="https://mindagent.github.io" class="button">Demo</a> <br />              
              &nbsp;&nbsp; We present <tt>MindAgent</tt>, an infrastructure for emergent HCI Spatial-Intelligence system, 
           enables<span style="color:#f1f0f1;"> <b>multi-agents optimization</b></span> and <span style="color:#f1f0f1;"> <b>human-agent interaction</b></span>.<br />
              &nbsp;&nbsp;&nbsp;– Proceedings of NAACL 2024. </div>
            <div> <b>■ Generactive AI </b>with Action-Vision-Language Model (AVL), vision-lanaguage model (VLM), and large-language-model (LLM). 
              <b>Vision-Language Pre-training:</b> Large multimodality pre-training model.
              <a href="https://arxiv.org/abs/2305.00970">[paper1]</a> <a href="https://arxiv.org/abs/2205.09256">[paper2]</a><br />
              &nbsp;&nbsp;&nbsp;– the model has been shipped to MS Product team, with Open AI.<br />
              &nbsp;&nbsp;&nbsp;– Proceedings of Transactions on Machine Learning Research (T-MLR). 2023
            </div>
            <div> <b>■ <b> <span style="color:#9c0a1c;"><i class="fa fa-database" aria-hidden="true"></i></b> Embodied Chatbot Agent of Interactive System, Data-driven Model and Benchmark for Mimicking Human Behavior, Cognition, and Affective-computing </b>with Trustworty Human Empathy and Society Value Alignment. NICE: Neural Image Commenting with Empathy. 
              <a href="https://nicedataset.github.io">[paper]</a> <a href="https://nicedataset.github.io" class="button">Demo</a> <br />
              &nbsp;&nbsp;&nbsp;– Proceedings of EMNLP 2021. <b> <span style="color:#9c0a1c;"><i class="fa fa-database" aria-hidden="true"></i> Large-scale Embodied Dataset, Benchmack, and Interactive System. </b> <br /> 
              &nbsp;&nbsp;&nbsp;– Embodied Modal and Interactive System shipped to the MS Azure Agent as <a href="https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)">"Contana"</a>.  </div>
            <div> <b>■ <span style="color:#9c0a1c;"><i class="fa fa-trophy" aria-hidden="true"></i></b> <b>Embodied AI Agent of autonomous HCI system with RL/IL/MPC:</b> Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation of autonomous HCI system robotics agent. 
             <a href="https://arxiv.org/pdf/1811.10092">[paper]</a> 
             <a href="https://www.microsoft.com/en-us/research/video/reinforced-cross-modal-matching-and-self-supervised-imitation-learning-for-vision-language-navigation/" class="button">Demo</a> <br />
              &nbsp;&nbsp;&nbsp;– Proceedings of CVPR 2019.  <b><span style="color:#9c0a1c;"><i class="fa fa-trophy" aria-hidden="true"></i> Best Student Paper Award.</b> </div> 
            <div> <b>■ Augmented AVL/LLM/VLM with logical and knowledge inference reasoning interpratibility </b>for Huamn-in-the-loop Interaction. 
              <a href="https://aclanthology.org/2023.findings-acl.111.pdf">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;– Proceedings of ACL 2023. </div>
          </p>
              
          <p>
            <b> <span style="color:#9dbad1;"><b> ♢ Algorithms: Embodied AI with Reinforcement Learning (RL), Imitation Learning (IL), Model Predictive Control (MPC), 
              Generative Adversarial Network (GAN), and Decision-Making </b></b>
            <div> <b>■ Embodied Autonomous Foundation Model for Generative AI with RL/IL/MPC</b>. <b>Generative AI (GenAI):</b> dynamic interaction model generation  
              in multimodality via emergent abilities with reinforcement learning and imitation learning.
              <a href="https://arxiv.org/abs/2305.00970">[paper1]</a> <a href="https://augmented-reality-knowledge.github.io" class="button">Demo</a>
              <a href="https://www.microsoft.com/en-us/research/uploads/prod/2021/10/kb_vlp_ICML2021.pdf">[paper2]</a> <a href="https://icml21ssl.github.io/pages/files/kb-vlp-poster.pdf" class="button">Poster</a> <br />
              &nbsp;&nbsp;&nbsp;– the model has been shipped to MS Product team, with Open AI.<br />
              &nbsp;&nbsp;&nbsp;– Proceedings of ICML 2021.</div>
            <div> <b>■ Generative AI for Human-in-the-loop intelligence of LLM/VLM with RL</b>. 
              <a href="https://arxiv.org/abs/1805.08191">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;– Proceedings of AAAI 2019. <b> <span style="color:#9c0a1c;"> Oral.</b> </div>
            <div> <b>■ Embodied AI with RL/IL/MPC:</b> Vision-Language Navigation Policy Learning and Adaptation for simulation agent robotics. 
              <a href="https://ieeexplore.ieee.org/document/8986691">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;– IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI) 2020.</div>
            <div> <b>■ Embodied Robotics for Generative AI in Multi-modality with RL/GAN:</b> Turbo Learning for CaptionBot and DrawingBot. 
              <a href="https://arxiv.org/abs/1805.08170">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;– Proceedings of NeurIPS 2018. <b> <span style="color:#9c0a1c;"> Oral.</b> </div>
            <div> <b>■ <span style="color:#9c0a1c;"><i class="fa fa-trophy" aria-hidden="true"></i></b> <b>Embodied Communication:</b> Martian–message broadcast via LED lights to heterogeneous smartphones. 
              <a href="https://dl.acm.org/doi/abs/10.1145/2973750.2985257">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;– Proceedings of ACM Mobicom. Poster. <span style="color:#9c0a1c;"> <b><i class="fa fa-trophy" aria-hidden="true"></i> Best Poster Paper Award.</b> </div>
                                                                                                                                                                                                                 
          </p>

      
          <p>                                                                                                        
          <b> <span style="color:#9dbad1;"><b> ♢ [Sim-to-Real] Autonomous HCI System: Dynamic System Control of Spatial and Temporal Interactive Intelligence for Neural-symblic Computation & Neuroscience/Cognition/Healthcare/Bionic Representation on Embodied Inference Interpretability 
            with Complexes Interdisciplinary Innovation </b></b>
            <div> <b>■ Embodied Dynamics Autonomous Systems</b>, which conducts research in modeling and control of distributed parameters systems, 
              with applications to Autonomous Navigation, Robotics Systems (action perdiction, manupulation, motion planning and coordination, aerospace vehicle, 
              state estimation and localization, etc.) . <a href="https://www.sanctuary.ai/blog/sanctuary-ai-announces-microsoft-collaboration-to-accelerate-ai-development-for-general-purpose-robots" class="button">Demo</a> <a href="https://arxiv.org/pdf/2402.05929">[paper]</a>  <br /></div>
            <div> <b>■ Infinite Dimensional Systems </b>for Humaniod Robotics, Aerospace Navigation, and Wearable Divice with Cognition and Bio-Neurocomputation (mimic human behavior, manupulation, and emotion) 
              on Complex Intelligence. <br />
              <a href="https://www.youtube.com/watch?v=iBfjx9R3FP8" class="button">Demo</a>  <a href="https://arxiv.org/abs/2305.00970">[paper1]</a> <a href="https://aclanthology.org/2024.findings-naacl.200.pdf">[paper2]</a>
               <a href="https://arxiv.org/abs/1711.10485">[paper3]</a> <a href="https://arxiv.org/abs/1902.10740">[paper4]</a> <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– the model has been shipped to MS Product team, with Open AI.<br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings of NAACL 2024. <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings of CVPR 2019. <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings of CVPR 2018. 
            </div> 
           <div> <b>■ Embodied AI of Neural-symbolic Computational Representation of Neural Network Inference Interpretation</b> (Healthcare, NeuroScience, Cognition) for Mimicking Human Behavior with Human Empathy Alignment in the cutting-edge frontiers research.
            <br />
               <div> &nbsp;&nbsp;i) <b><span style="color:#9c0a1c;"><i class="fa fa-trophy" aria-hidden="true"></i></b> <b> Tensor Product Representation of Inference Neural-Symbolic Foundation Model with Neuroscience Interpretability.</b> <a href="https://arxiv.org/abs/1910.02339v2">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings of ICML 2020. <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings of NeurIPS workshop 2019. <span style="color:#9c0a1c;"> <b> <i class="fa fa-trophy" aria-hidden="true"></i> Best Paper Award.</b></div>
              <div> &nbsp;&nbsp;ii) Neural-Symbolic of Inference Intelligence in Generative AI: Attentive Tensor Product Learning;<a href="https://arxiv.org/abs/1802.07089">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings of AAAI 2019 </div> 
              <div> &nbsp;&nbsp;iii) Neural-Symbolic Representation of Tensor Product Generative AI: Tensor Product Generation Networks for Deep NLP Modeling; 
              <a href="https://aclanthology.org/N18-1114/">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings of NAACL 2018. </div>
            </div>  
          </p>

          <p>
          <b> <span style="color:#9dbad1;"><b>♢ Trustworthy Data-driven Distributed Machine Learning and Multi-agent Optimization: Trustworthy Dataverse Interactive Learning and Multimedia Networking in Latent Space with Trustworthy Huamn Empathy and Society Value Alignment</b></b>
            <div> <b>■ Knowledge Inference Reasoning Representation </b>for Large Foundation Models in Cognition Complex with Trustworthy Dataverse
              (mimic human behavior and inference-interpretability). 
              <br />
              &nbsp;&nbsp;&nbsp;– parts models has been shipped to MS Product team, with Open AI. 
              <div> &nbsp;&nbsp;i) Spatial AI: Retrieve What You Need: Post-Training for knowledge agent with RL.<a href="https://aclanthology.org/2024.tacl-1.14/">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings in Transactions of ACL (T-ACL), 2024 </div> 
              <div> &nbsp;&nbsp;ii) Localized Symbolic Knowledge Distillation for Visual Commonsense Models in Post-Training via ChatGPT. 
              <a href="https://arxiv.org/pdf/2312.04837">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings of NeurIPS 2023. </div>
              <div> &nbsp;&nbsp;iii) Logical Transformer for neurocumputation Neurocomputation Interpretability Representation.  <a href="https://aclanthology.org/2023.findings-acl.111.pdf">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings of ACL 2023. </div>
              <div> &nbsp;&nbsp;iv) <b><span style="color:#9c0a1c;"><i class="fa fa-trophy" aria-hidden="true"></i></b> <b> KAT: A Knowledge Augmented Transformer for Vision-and-Language.</b>  <a href="https://arxiv.org/pdf/2112.08614">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings of NAACL 2022. <span style="color:#9c0a1c;"> <b><i class="fa fa-trophy" aria-hidden="true"></i> No.1 State-of-The-Art (SoTA) Stage of Multimodality (OK-VQA) Leaderboard 2022.</b></div> 
              </div>
            <div> <b>■ Embodied AI of Data-driven computational and evaluation representation in distributed system governance</b>
              of across-disciplinary Boundaries.  
              <div> &nbsp;&nbsp;i) “TIGEr: Text-to-Image Grounding for Image Caption Evaluation”. <a href="https://arxiv.org/abs/1909.02050">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings in EMNLP, 2019 </div> 
             <div> &nbsp;&nbsp;ii) “REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning”. <a href="https://arxiv.org/abs/1909.02217">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings in EMNLP, 2019 </div> 
            </div>
      <div> <b>■ Embodied Computer Engineering in Trustworthy Multimedia Networking of Human-in-the-loop Distributed Machine Learning in Cutting-edge Frontiers Research. </b>
              <div> &nbsp;&nbsp;i) "Vision and Challenges for Knowledge Centric Networking”. <a href="https://ieeexplore.ieee.org/abstract/document/8685777">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings in IEEE Wireless Communications, 2018 </div> 
             <div> &nbsp;&nbsp;ii) “Trickle Irrigation: Congestion Relief for Communication with Network Coding”. <a href="https://ieeexplore.ieee.org/document/8373732">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings of IEEE Transactions on Vehicular Technology. 2017 </div> 
              <div> &nbsp;&nbsp;iii) "Just fun: A joint fountain coding and network coding approach to loss-tolerant information spreading”. <a href="https://mm.aueb.gr/research/mobihoc14/mobihoc/p83.pdf">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings in ACM MobiHoc, 2014 </div> 
              <div> &nbsp;&nbsp;iv) “Social networking reduces peak power consumption in smart grid”. <a href="http://staff.ustc.edu.cn/~xiangyangli/paper/Journal/SocialSmartGrid-TG.pdf">[paper]</a> <br />
              &nbsp;&nbsp;&nbsp;&nbsp;– Proceedings of IEEE Transactions on Smart Grid. 2014 </div> 
            </div>         
            
            </p>
         
        </p>

        <p> 
          During the past year, my research at MSR has centered around the theme of Infinite Embodied AI 
         for pre-trained large multimodal foundation models — an autonomous system that integrates multi-models into interactive agnet with neurocomputation inference knowledge
          on trustworthy value alignment.
          I believe, by integrating embodied AI with automous system agent on spatail and temporal distribution information, we can achieve better yet interpretable, grounded and robust multi-modality intelligence for cross-modality and agnostic-reality.
          Beyond pushing state-of-the-art research, I am a strong proponent of efficient and reproducible research. We also helped to ship Embodied AI techniques to Microsoft products and to create real-world impact.
          I enjoy the process of bridging the gap between theory and practice, bringing theoretical results to practical applications. 
          I also enjoy coding, building real systems from scratch, and making them work simply, efficiently, and elegantly. 
        </p>
      
      <!--h3 id="Talks">Recent Talks</h3>-->
        <!--h4 id="Task1">My recent talks at academia & industry for the research and practical applications involving:</h4>-->

      <!--p>
          <tt>• Nov 2024:</tt> Talk on <a>Embodied AI with Spatial and Temporal Intelligence Alignment</a> at Google Research. 
          </p>-->
      <!--p>
          <tt>• Nov 2024:</tt> Talk on <a>Embodied Autonomous System for Robotics and Navigation</a> at Waymo Research.
          </p>-->

       <!--p>
          <tt>• Oct 2024:</tt> Talk on <a>Embodied AI with Multimodal Agent</a> at Cambridge Univeristy. 
          </p>-->

         <!--p>
          <tt>• Aug-Sep 2024:</tt> Talk on <a>Embodied AI </a> at Harvard Univeristy.
          <p style="text-align: justify; font-size: 1.0em;">&nbsp;&nbsp;&nbsp; - Talk on <tt>Embodied AI</tt>, with 
           enables<span style="color:#f1f0f1;"> <b>Human Empathy</b></span> and <span style="color:#f1f0f1;">
           <b>Socity Value alignment</b></span>.
          </p>
          <p style="text-align: justify; font-size: 1.0em;">&nbsp;&nbsp;&nbsp; - Talk on <tt>AI Agnet</tt>, for
           enables<span style="color:#f1f0f1;"> <b>human-in-the-loop intelligence innovation</b></span> with <span style="color:#f1f0f1;">
           <b>interdisciplinary research</b></span>.
          </p>      
          </p>-->

        <!--p>
          <tt>• Aug 2024:</tt> Talk on <a>Dynamic Interaction Generative AI (GenAI) in Robotics and Multimodality </a> at Princeton University.
        </p>-->
      
        <!--p>
          <tt>• Jul 2024:</tt> Talk on <a>Infinite Embodied Spatial Intelligence in Real and Virtual World </a> at Stanford University.            
        </p>--> 

        <!--p>
          <tt>• May 2024:</tt> Talk on <a>"Embodied Per-training Foudation Model"</a> at Microsoft Research AI Frontiers.       
         </p>--> 

         <!--p>
          <tt>• Mar 2024:</tt> Talk on <a>"Embodied Agent Foudation Model"</a> at Project Green at Microsoft.       
         </p>--> 

        <!--p>
          <tt>• Sep 2023:</tt> Talk on <a>"Autonomous Multi-model Agent Interaction System"</a> at the Research Summit at Microsoft Research: 
          <div>          
          &nbsp;&nbsp;&nbsp;- Talk at MS Mesh team; the model has been shipped to MS office teams.
          </div> 
          <div>          
          &nbsp;&nbsp;&nbsp;- Talk at MS Gaming team; Robotics Team.
          </div>
          <div>          
          &nbsp;&nbsp;&nbsp;- Talk at MS Turing Team and Bing search; the model has been shipped to MS Azure teams.
          </div>        
        </p>--> 
      
        <!--p>
          <tt>• Oct 2022:</tt> Talk on <a>Panel for "Large-scale Embodied AI "</a>at Microsoft Research Summit.
        </p>--> 
      
          <!--p>
          <b>• Embodied Action Foundation Model:</b> Large embodied foundation models for flexible AI agent infrastructure with vision-language. 
          "An Interactive Agent Foundation Model" of large action-vision-language per-training for embodied AI in Robotics, Gaming/mix-reality, 
          and Embodied Healthcare. The model developed in collaboration with MS Product Robotics team, Gaming Team, Turing team, and Stanford.
          <a href="https://agentfoundationmodel.github.io/pdfs/paper.pdf" class="button">Link</a>
          <a href="https://agentfoundationmodel.github.io" class="button">Demo Link</a>
           
          </p>-->

         <!--p>
          <b>• Multi-agent Infrastructure with In-contextual Learning:</b> "MindAgent: multi-agent infrastructure with GPT-4 (V) ison 
          for infinite spatial intelligence in real and virtual world", in collaboration with MS X-box team, and shipped to MS Gaming team; 
           <a href="https://mindagent.github.io" class="button">Demo Link</a>
      
          <p style="text-align: justify; font-size: 1.0em;">We present <tt>MindAgent</tt>, an infrastructure for emergent gaming interaction, 
           enables<span style="color:#f1f0f1;"> <b>multi agents collaboration</b></span> and <span style="color:#f1f0f1;">
           <b>human-agent collaboration</b></span>.
          </p>      
          </p>-->

        <!--p>
          <b>• Post-Training for Generative AI:</b> dynamic interaction model generation (GenAI) in multimodality via emergent abilities. "ArK: 
          Augmented reality with emergent infrastructure". Post-training using reinforcement and imitation learning for generative AI (GPT, Dall-E), 
          in collaboration with MS Mesh team; the model has been shipped to MS office teams;
                 
        <a href="https://arxiv.org/pdf/2305.00970" class="button">Link</a>
               
        </p>-->
      
        <!--p>
          <b>• Applications for Human-Machine Interaction:</b> Embodied Multimodal Navigation with RL & IL. "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning 
          for Vision-Language Navigation" for simulation agent robotics, in collaboration with MS Cognition Services team;
                  
        <a href="https://arxiv.org/pdf/1811.10092" class="button">Link</a>
         <a href="https://www.microsoft.com/en-us/research/video/reinforced-cross-modal-matching-and-self-supervised-imitation-learning-for-vision-language-navigation/" class="button">Demo Link</a>                
        </p>-->  

         <!--p>
          <b>• Vision-Language Pre-training:</b> "Training Vision-Language Transformers from Captions", 
           Vision-language pre-training model generation in collaboration with Turing team and Cognition Services;
                   
         <a href="https://arxiv.org/abs/2205.09256" class="button">Link</a>
                 
         </p>--> 

        <!--p>
          <b>• Knowledge Inference and Representation in Multi-modality:</b> Knowledge-Inference Reasoning Transformer: 
          <div>          
          i) "KAT: A Knowledge Augmented Transformer for Vision-and-Language", <a href="https://arxiv.org/abs/2112.08614" class="button">Link</a>
          </div> 
          <div>          
          ii) "Retrieve What You Need: knowledge-LLM agent with GPT", <a href="https://aclanthology.org/2024.tacl-1.14/" class="button">Link</a>
          </div>
          <div>          
          iii) "Logical Transformer", which collaborated with MS Turing Team and Bing search. 
          <a href="https://aclanthology.org/2023.findings-acl.111.pdf" class="button">Link</a>
          </div>        
        </p>--> 
              
        
        <h3 id="Publications">Selected Publications   </h3>  
        <h4 id="Task1">(please refer to <a href="https://scholar.google.com/citations?user=U7Mmyc8AAAAJ&hl=en" class="button">Google Scholar</a> for the full publications)   
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   【<i class="fa fa-asterisk"></i> Equal Contribution. <i class="fa fa-flag-o" aria-hidden="true"></i> Project Lead.】</h4>          
           
        <p>
          <b>■ An Interactive Agent Foundation Model</b> for embodied interaction in Robot, Gaming, and Healthcare.
        <div>
          Z. Durante<i class="fa fa-asterisk" style="font-size:12px"></i>, R. Gong<i class="fa fa-asterisk" style="font-size:12px"></i>, R. Taori, Y. Noda, P. Tang, 
          E. Adeli, S. Kowshika Lakshmikanth, K. Schulman, A. Milstein, D. Terzopoulos, A. Famoti, 
          N. Kuno, A. Llorens, H. Vo, K. Ikeuchi, L. Fei-Fei, J. Gao, N. Wake<i class="fa fa-asterisk" style="font-size:12px"></i><i class="fa fa-flag-o" aria-hidden="true"></i>, <b><u>Q. Huang<i class="fa fa-asterisk" style="font-size:12px"></i><i class="fa fa-flag-o" aria-hidden="true"></i></u></b>.  <br /> 
          <i class="fa fa-asterisk" style="font-size:12px"></i>Equal Contribution. <i class="fa fa-flag-o" aria-hidden="true"></i> Project Lead.
        </div>          
          arXiv:2402.05929, May 2024.
          <a href="https://arxiv.org/pdf/2402.05929">[paper]</a>
          <a href="https://agentfoundationmodel.github.io">[webpage]</a>           
         </p>

        <p>
          <b>■ Agent AI Towards a Holistic Intelligence.</b>
        <div>
          <b><u>Q. Huang</u></b>, N. Wake, Z. Durante, R. Gong, R. Taori, Y. Noda, D. Terzopoulos, 
          N. Kuno, A. Famoti, A. Llorens, J. Langford, H. Vo, L. Fei-Fei, K. Ikeuchi, J. Gao.        
        </div>          
          arXiv:2403.00833, May 2024.          
          <a href="https://arxiv.org/abs/2403.00833">[paper]</a>        
    </p>

        <p> 
          <b>■ MindAgent: Emergent Gaming Interaction.</b>
        <div>R. Gong<i class="fa fa-asterisk" style="font-size:12px"></i>, <b><u>Q. Huang<i class="fa fa-asterisk" style="font-size:12px"></i><i class="fa fa-flag-o" aria-hidden="true"></i></u></b>, X. Ma<i class="fa fa-asterisk" style="font-size:12px"></i>, H. Vo, Z. Durante, Y. Noda, Z. Zheng, 
          S. Zhu, D. Terzopoulos, L. Fei-Fei, J. Gao.  
          <i class="fa fa-asterisk" style="font-size:12px"></i>Equal Contribution. <i class="fa fa-flag-o" aria-hidden="true"></i> Project Lead.        
        </div>          
        Proceedings of NAACL 2024, Jun. 2024.          
          <a href="https://arxiv.org/pdf/2309.09971">[paper]</a>
          <a href="https://mindagent.github.io">[webpage]</a>         
    </p>  

    <p> 
          <b>■ Agent AI: Surveying the Horizons of Multimodal Interaction.</b>
        <div>Z. Durante<i class="fa fa-asterisk" style="font-size:12px"></i>, <b><u>Q. Huang<i class="fa fa-asterisk" style="font-size:12px"></i><i class="fa fa-flag-o" aria-hidden="true"></i></i></u></b>, N. Wake<i class="fa fa-asterisk" style="font-size:12px"></i>, R. Gong, J. Park, B. Sarkar, 
          R. Taori, Y. Noda, D. Terzopoulos, Y. Choi, K. Ikeuchi, H. Vo, L. Fei-Fei, J. Gao.  <br /> 
          <i class="fa fa-asterisk" style="font-size:12px"></i>Equal Contribution. <i class="fa fa-flag-o" aria-hidden="true"></i> Project Lead.</div>          
        arXiv:2401.03568, Jan 2024.
          <a href="https://arxiv.org/abs/2401.03568">[paper]</a>         
    </p> 

  <p> 
          <b>■ Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering.</b>
        <div>D. Wang, <b><u>Q. Huang</u><i class="fa fa-flag-o" aria-hidden="true"></i></b>, M. Jackson, J. Gao</div>          
        Proceedings of Transactions of ACL (TACL) 2024. Apr. 2024
      <a href="https://aclanthology.org/2024.tacl-1.14/">[paper]</a>         
    </p>       

    <p> 
          <b>■ ArK: Augmented Reality with Knowledge Interactive Emergent Ability. (Generative AI)   </b>
        <div> <b><u>Q. Huang</u></b>, J. Park, A. Gupta, P. Bennett, R. Gong, S. Som, B. Peng, O. Mohammed, C. Pal, Y. Choi, J. Gao.         
        </div>          
        arXiv:2305.00970. Shipped the model in Microsoft Teams for product, collobarated with Microsoft Mesh team and Turing team. Jun. 2023
      <a href="https://arxiv.org/abs/2305.00970">[paper]</a> 
      <a href="https://augmented-reality-knowledge.github.io">[webpage]</a>
      <a href="https://www.microsoft.com/en-us/research/project/mixed-reality/">[product page]</a> 
    </p>

      

    <p> 
          <b>■ Localized Symbolic Knowledge Distillation: Infinite Knowledge Distillation in Multi-modality (Knowledge-auto-GPT)</b>
        <div>J. Park, J. Hessel, K. Chandu, P. Liang, X. Lu, P. West, Y. Yu, <b><u>Q. Huang</u></b>, 
          J. Gao, A. Farhadi, Y. Choi
        </div>          
        Proceedings of NeurIPS 2023. Dec.2023
          <a href="https://arxiv.org/abs/2312.04837">[paper]</a>        
    </p>  

        <p> 
          <b>■ Logical Transformers: Infusing Logical Structures into Pre-Trained Language Models</b> 
        <div>B. Wang, <b><u>Q. Huang<i class="fa fa-flag-o" aria-hidden="true"></i></u></b>, B. Deb, A. Halfaker, L. Shao, D. McDuff, A. Awadallah, D. Radev, J. Gao</div>          
          Proceedings of ACL 2023. Jul.2023
            <a href="https://aclanthology.org/2023.findings-acl.111.pdf">[paper]</a>   
      </p> 

        <p> 
          <b>■ Training Vision-Language Transformers from Captions (Vision-Lanaguage Pertraining)</b> 
        <div>L. Gui, Y. Chang, <b><u>Q. Huang<i class="fa fa-flag-o" aria-hidden="true"></i></u></b>, S. Som, A. Hauptmann, J. Gao, Y. Bisk.</div>          
          Proceedings of Transactions on Machine Learning Research. 2023
      <a href="https://arxiv.org/abs/2205.09256">[paper]</a>   
      </p> 

      
      
    <p> 
          <b>■ <b> <span style="color:#9c0a1c;"><i class="fa fa-trophy" aria-hidden="true"></i> </b> <b> KAT: A Knowledge Augmented Transformer for Vision-and-Language.</b> <span style="color:#9c0a1c;"><b><i class="fa fa-trophy" aria-hidden="true"></i> No.1 State-of-The-Art (SoTA) Stage of Multimodality (OK-VQA) Leaderboard (2022).</b>
        <div>L. Gui, B. Wang, <b><u>Q. Huang<i class="fa fa-flag-o" aria-hidden="true"></i></u></b>, A. Hauptmann, Y. Bisk, J. Gao
        </div>          
        Proceedings of NAACL 2022, Jun. 2022.  
          <a href="https://arxiv.org/abs/2112.08614">[paper]</a>   
      </p> 

    <p> 
          <b>■ <b> <span style="color:#9c0a1c;"><i class="fa fa-database" aria-hidden="true"></i></b> <b> NICE: Neural Image Commenting with Empathy.</b> <br />     
           <b> <span style="color:#9c0a1c;"><i class="fa fa-database" aria-hidden="true"></i> New Large-scale Per-training Dataset and Benchmack </b> for Embodied Multimodality and Interactive System with Human Value Alignment.</b>
        <div>K. Chen, <b><u>Q. Huang<i class="fa fa-flag-o" aria-hidden="true"></i></u></b>, D. McDuff, X. Gao, H. Palangi, J. Wang, K. Forbus, J. Gao
        </div>          
        Proceedings of EMNLP 2021, Jan. 2021.  
          <a href="https://aclanthology.org/2021.findings-emnlp.380.pdf">[paper]</a>   
      </p> 


   <p> 
          <b>■ Vision-Language Navigation Policy Learning and Adaptation.</b> 
        <div>X. Wang, <b><u>Q. Huang<i class="fa fa-flag-o" aria-hidden="true"></i></u></b>, A. Celikyilmaz, J. Gao, D. Shen, Y. Wang, W. Wang, L. Zhang
        </div> 
        IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI). Jun. 2020. <a href="https://ieeexplore.ieee.org/document/8986691">[paper]</a>    
       </p>      
 
       <p> 
          <b>■ <b> <span style="color:#9c0a1c;"><i class="fa fa-trophy" aria-hidden="true"></i> </b> <b> Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation.</b> 
            <span style="color:#9c0a1c;"><b><i class="fa fa-trophy" aria-hidden="true"></i> Best Student Paper Award.</b>
        <div>X. Wang, <b><u>Q. Huang<i class="fa fa-flag-o" aria-hidden="true"></i></u></b>, A. Celikyilmaz, J. Gao, D. Shen, Y. Wang, W. Wang, L. Zhang
        </div> 
        Proceedings of CVPR 2019. Jun. 2019. <a href="https://arxiv.org/abs/1811.10092">[paper]</a>    
       </p>  


       <p> 
          <b>■ Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations.</b>
        <div> K. Chen, <b><u>Q. Huang<i class="fa fa-flag-o" aria-hidden="true"></i></u></b>, H. Palangi, P. Smolensky, K. Forbus, J. Gao
        </div>          
         Proceedings of ICML 2020. Feb.2020          
      <a href="https://arxiv.org/abs/1910.02339">[paper]</a>   
      </p>  

      <p> 
          <b>■ <b> <span style="color:#9c0a1c;"><i class="fa fa-trophy" aria-hidden="true"></i></b> <b> TP-N2F: Tensor Product Representation for Natural To Formal Language Generation.</b> <span style="color:#9c0a1c;"><b><i class="fa fa-trophy" aria-hidden="true"></i> Best Paper Award.</b>
        <div>K. Chen, <b><u>Q. Huang<i class="fa fa-flag-o" aria-hidden="true"></i></u></b>, H. Palangi, P. Smolensky, K. Forbus, J. Gao. </div>          
          Proceedings of NeurIPS workshop 2019. Dec.2019.  
          <a href="https://arxiv.org/abs/1910.02339v2">[paper]</a>   
      </p>  

       <p> 
          <b>■ TIGEr: Text-to-Image Grounding for Image Caption Evaluation.</b>
        <div>M. Jiang, <b><u>Q. Huang<i class="fa fa-flag-o" aria-hidden="true"></i></u></b>, L. Zhang, X. Wang, P. Zhang, Z. Gan, J. Diesner, J. Gao
        </div>          
        Proceedings of EMNLP 2021, Apr. 2019.  
          <a href="https://arxiv.org/abs/2112.08614">[paper]</a>   
      </p> 

      <p> 
          <b>■ REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning.</b>
        <div>M. Jiang, J. Hu, <b><u>Q. Huang</u></b>, L. Zhang, J. Diesner, J. Gao
        </div>          
        Proceedings of EMNLP 2021, Apr. 2019.  
          <a href="https://arxiv.org/abs/2112.08614">[paper]</a>   
    </p> 
            

       <p> 
          <b>■ Object-driven Text-to-Image Synthesis via Adversarial Training.</b>
        <div>W. Li, P. Zhang, L. Zhang, <b><u>Q. Huang</u></b>, X. He, S. Lyu, J. Gao
        </div>          
        Proceedings of CVPR 2019, Jun. 2019.  
          <a href="https://arxiv.org/abs/1902.10740">[paper]</a>   
    </p> 


       <p> 
          <b>■ Attentive Tensor Product Learning.</b> 
        <div> <b><u>Q. Huang</u></b>, L. Deng, O. Wu, C. Liu, X.He</div>          
          Proceedings of AAAI 2019. Feb. 2019
          <a href="https://arxiv.org/abs/1802.07089">[paper]</a>   
      </p> 

       <p> 
          <b>■ Turbo Learning for CaptionBot and DrawingBot.</b> 
        <div> <b><u>Q. Huang</u></b>, P. Zhang, D. Wu, L. Zhang.</div>          
          Proceedings of NeurIPS 2018. Dec.2018.  
          <a href="https://arxiv.org/abs/1805.08170">[paper]</a>   
      </p>  

        <p> 
          <b>■ Hierarchically Structured Reinforcement Learning for Topically Coherent Visual (video) Story Generation.</b> 
        <div> <b><u>Q. Huang*</u></b>, Z. Gan*, A. Celikyilmaz, L. Li, D. Wu, X. He. *Equal contribution. </div>          
          Proceedings of AAAI 2019. Feb.2019          
       <a href="https://arxiv.org/abs/1805.08191">[paper]</a>   
      </p> 

         <p> 
          <b>■ Attentive Tensor Product Learning.</b> 
        <div> <b><u>Q. Huang</u></b>, L. Deng, O. Wu, C. Liu, X. He.</div>          
          Proceedings of AAAI 2019. Feb.2019          
      <a href="https://arxiv.org/abs/1802.07089">[paper]</a>   
      </p>


         <p> 
          <b>■ AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks.</b> 
        <div>T. Xu, P. Zhang<i class="fa fa-flag-o" aria-hidden="true"></i>, <b><u>Q. Huang<i class="fa fa-flag-o" aria-hidden="true"></i></u></b>, H. Zhang, Z. Gan, X. Huang, X. He.</div>          
          Proceedings of CVPR 2018. Jun.2018          
      <a href="https://arxiv.org/abs/1711.10485">[paper]</a>   
      </p> 

       <p> 
          <b>■ Tensor Product Generation Networks for Deep NLP Modeling.</b> 
        <div> <b><u>Q. Huang</u></b>, P. Smolensky, X. He, L. Deng, D. Wu, Long paper.</div>          
          Proceedings of NAACL 2018, ACL–Association for Computational Linguistics, New Orleans, Louisiana, US. Jun.1- Jun.6. 2018.
     <a href="https://aclanthology.org/N18-1114/">[paper]</a>   
      </p> 


          <p> 
          <b>■ <b> <span style="color:#9c0a1c;"><i class="fa fa-trophy" aria-hidden="true"></i></b> <b> Martian--message broadcast via LED lights to heterogeneous smartphones: poster.</b> <span style="color:#9c0a1c;"><b><i class="fa fa-trophy" aria-hidden="true"></i> Best Poster Paper Award.</b>        
            <div> H. Du, J. Han, <b><u>Q. Huang</u></b>, X. Jian, C. Bo, Y. Wang, X.-Y. Li.</div>          
          Proceedings of the 22nd ACM MobiCom, 417-418. Feb. 2016
         <a href="https://dl.acm.org/doi/abs/10.1145/2973750.2985257">[paper]</a>   
          </p> 

           <p> 
          <b>■ Job Scheduling Under Differential Pricing: Hardness and Approximation Algorithms.</b>    
            <div> <b><u>Q. Huang</u></b>, J. Zhao, H. Du, J. Hou, X.-Y. Li.</div>          
          Proceedings of the IEEE WASA, 417-418. Dec. 2016
         <a href="https://link.springer.com/chapter/10.1007/978-3-319-60033-8_55">[paper]</a>   
          </p>
      

         <p> 
          <b>■ Just fun: A joint fountain coding and network coding approach to loss-tolerant information spreading. </b>        
            <div> <b><u>Q. Huang</u></b>, K. Sun, X. Li, O. Wu.</div>          
          Proceedings of the ACM MobiHoc. Aug. 2014
         <a href="https://mm.aueb.gr/research/mobihoc14/mobihoc/p83.pdf">[paper]</a>   
          </p> 


         <p> 
          <b>■ Social networking reduces peak power consumption in smart grid.</b>       
            <div> <b><u>Q. Huang</u></b>, X. Li, J. Zhao, O. Wu, X.-Y. Li.</div>          
          Proceedings of IEEE Transactions on Smart Grid. Dec. 2014
         <a href="http://staff.ustc.edu.cn/~xiangyangli/paper/Journal/SocialSmartGrid-TG.pdf">[paper]</a>   
          </p> 

          <p> 
          <b>■ Smoothing the energy consumption: Peak demand reduction in smart grid.</b>       
            <div> S. Tang, <b><u>Q. Huang</u></b>, X. Li, O. Wu.</div>          
          Proceedings of IEEE INFOCOM. Apr. 2013
         <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ef3ad97b671c4b94338cc7d37d99b24ce0522f14">[paper]</a>   
          </p> 

          <p> 
          <b>■ Flatten a Curved Space by Kernel: From Einstein to Euclid.</b>       
            <div> <b><u>Q. Huang</u></b>, O. Wu.</div>          
          Proceedings of IEEE Signal Processing Magazine [Impact Factor: 9.4]. Sep. 2013
         <a href="http://www.wu.ece.ufl.edu/mypapers/Kernel-basedLearning_technicalReport.pdf">[paper]</a>   
          </p> 

          <p> 
          <b>■ SilentSense: silent user identification via touch and movement behavioral biometrics.</b>       
            <div> C. Bo, L. Zhang, X.Li, <b><u>Q. Huang</u></b>, Y. Wang.</div>          
          Proceedings of IEEE Mobicom. Sep. 2013
         <a href="https://web.archive.org/web/20170706064256id_/http://webpages.uncc.edu/ywang32/research/mobicomPOS06-bo.pdf">[paper]</a>   
          </p> 


      


      
      
      
      <h3 id="ToolDownload">Trustworthy Data Collection and Analyzation</h3>
        <p>
        I am also working on the trustworthy data-driven alignment for large-scale embodied multimodal data collection, safety analysis, and related benchmark generation. 
        It collaborated with MS Turing team, Stanford University, and Open AI.
        </p>
        <p>
          <b>Data Collection and Analyzation: </b> our released <br> 
          ■ New Dataset and Banchmark of Embodied AI with Human Value Aligment: <br /> 
          <a href="https://nicedataset.github.io">“NICE” dataset and banchmark: embodied data-driven multimodality for mimic human action, enmotion, with human empathy and sccity value alignment.</a> <br />
          ■ New Dataset and Banchmark, and HCI System of Multi-Agent MPC and Optimization: <br />
          <a href="https://github.com/mindagent/mindagent">“Cuisine world” dataset, benchmark, and infrastructure: embodied multi-agent data-driven for human-machine-interaction in Spatial and Temporal Intelligence</a>. <br /> 
          You can find the codes for data collection below: 
        </p>
        <p><a href="https://github.com/NICEdataset/DataCollectionTool" class="button">Data Collection Tools</a></p>
       
      
      <h3 id="EULA">User License Agreement (EULA) for Using Our Dataset</h3>
        <p>
          You should complete an end user license agreement (EULA) before accessing the dataset/benchmark/infractrcuture. <br />
          The EULA will be posted soon once the data releasing process is finished. 
        </p>
        <a href="https://github.com/nicedataset/NICEdataset.github.io/blob/master/End%20User%20License%20Agreement.pdf" class="button">Download EULA</a>
      
      
      <h3 id="Fun">Fun</h3>
        <p>
        <p>Ballet, Violin, Tennis, Hiking</p>
        </p>
        
        
    </div>
  </body>

</html>
